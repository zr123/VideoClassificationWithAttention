{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f4b98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from IPython.core.display import Image\n",
    "\n",
    "from VCWA import Models, AttentionModels, Common, VideoDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a7b37b",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a6ddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount cloud-storage bucket\n",
    "# !mkdir /home/jupyter/bucket\n",
    "!gcsfuse --implicit-dirs gfr-master-data-bucket /home/jupyter/bucket/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743bad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 192 # use high batch size only for single-frame batches\n",
    "test_batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d221cca",
   "metadata": {},
   "source": [
    "### HMDB51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c70ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_no = 1\n",
    "datasetname = \"hmdb51\"\n",
    "classes = 51\n",
    "path = \"/home/jupyter/\"\n",
    "\n",
    "dataset = Common.get_dataset(\n",
    "    path + \"processed_datasets/hmdb51_vid25\", \n",
    "    path + \"datasets/hmdb51_org_splits\", \n",
    "    path + \"processed_datasets/hmdb51_optflowl10_npz25\", \n",
    "    split_no, \n",
    "    \"hmdb51\"\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3205f5f0",
   "metadata": {},
   "source": [
    "### UCF-101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de80dd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_no = 1\n",
    "datasetname = \"ucf101\"\n",
    "classes = 101\n",
    "path = \"/home/jupyter/\"\n",
    "\n",
    "dataset = Common.get_dataset(\n",
    "    path + \"processed_datasets/ucf101_vid25\", \n",
    "    path + \"datasets/ucfTrainTestlist\", \n",
    "    path + \"processed_datasets/ucf101_optflowl10_npz25\", \n",
    "    split_no, \n",
    "    \"ucf101\"\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83068c3",
   "metadata": {},
   "source": [
    "## TODO: 2D-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da298448",
   "metadata": {},
   "source": [
    "## TwoStream-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5500a5",
   "metadata": {},
   "source": [
    "### Pre-Training individual Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e583dc",
   "metadata": {},
   "source": [
    "#### Video Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecc3ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_train_gen = VideoDataGenerator.VideoDataGenerator(\n",
    "    dataset,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=train_batch_size,\n",
    "    preprocessing_function=tf.keras.applications.resnet_v2.preprocess_input,\n",
    "    shape_format=\"images\",\n",
    "    single_frame=True,\n",
    "    rotation_range=20.0,\n",
    "    shear_range=20.0,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "video_test_gen = VideoDataGenerator.VideoDataGenerator(\n",
    "    dataset,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=test_batch_size,\n",
    "    preprocessing_function=tf.keras.applications.resnet_v2.preprocess_input,\n",
    "    shape_format=\"images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5652ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "# video_model = tf.keras.models.load_model(\"models/twostream_25_L10/ResNet50v2/video\")\n",
    "\n",
    "# Create new\n",
    "# video_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3), classes=classes, weights=None)\n",
    "# video_model = AttentionModels.create_ResidualAttention_MobileNetV2(input_shape=(224, 224, 3), classes=classes)\n",
    "video_model = AttentionModels.create_CBAM_MobileNetV2(input_shape=(224, 224, 3), classes=classes)\n",
    "\n",
    "video_model.compile(\n",
    "    loss=\"categorical_crossentropy\", \n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9, decay=0.0001), \n",
    "    metrics=[\"accuracy\", tf.keras.metrics.TopKCategoricalAccuracy(5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf7dd15",
   "metadata": {},
   "source": [
    "#### OptFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30286c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optflow_dataset = dataset.copy()\n",
    "del optflow_dataset[\"path\"]\n",
    "optflow_dataset.rename(columns = {\"optflow_path\": \"path\"}, inplace=True)\n",
    "\n",
    "optflow_train_gen = VideoDataGenerator.VideoDataGenerator(\n",
    "    optflow_dataset,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=train_batch_size,\n",
    "    preprocessing_function=None,\n",
    "    shape_format=\"images\",\n",
    "    single_frame=True,\n",
    "    rotation_range=20.0,\n",
    "    shear_range=20.0,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "optflow_test_gen = VideoDataGenerator.VideoDataGenerator(\n",
    "    optflow_dataset,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=test_batch_size,\n",
    "    preprocessing_function=None,\n",
    "    shape_format=\"images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5c6964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "# video_model = tf.keras.models.load_model(\"models/twostream_25_L10/ResNet50v2/optflow\")\n",
    "\n",
    "# Create new\n",
    "# optflow_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 20), classes=classes, weights=None)\n",
    "# optflow_model = AttentionModels.create_ResidualAttention_MobileNetV2(input_shape=(224, 224, 20), classes=classes)\n",
    "optflow_model = AttentionModels.create_CBAM_MobileNetV2(input_shape=(224, 224, 20), classes=classes)\n",
    "\n",
    "optflow_model.compile(\n",
    "    loss=\"categorical_crossentropy\", \n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9, decay=0.0001), \n",
    "    metrics=[\"accuracy\", tf.keras.metrics.TopKCategoricalAccuracy(5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a959150c",
   "metadata": {},
   "source": [
    "#### TwoStream Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99130d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "twostream_train_gen = VideoDataGenerator.VideoDataGenerator(\n",
    "    dataset,\n",
    "    target_size=(224, 224),\n",
    "    optflow=True,\n",
    "    batch_size=train_batch_size,\n",
    "    preprocessing_function=tf.keras.applications.resnet_v2.preprocess_input,\n",
    "    single_frame=True,\n",
    "    rotation_range=20.0,\n",
    "    shear_range=20.0,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "twostream_test_gen = VideoDataGenerator.VideoDataGenerator(\n",
    "    dataset,\n",
    "    target_size=(224, 224),\n",
    "    optflow=True,\n",
    "    batch_size=test_batch_size,\n",
    "    preprocessing_function=tf.keras.applications.resnet_v2.preprocess_input\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec6cf3f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efcab0a",
   "metadata": {},
   "source": [
    "### Combined Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146aa48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Models.train_optflow_model(\n",
    "    video_model,\n",
    "    optflow_model,\n",
    "    video_train_gen,\n",
    "    video_test_gen,\n",
    "    optflow_train_gen,\n",
    "    optflow_test_gen,\n",
    "    twostream_test_gen,\n",
    "    iterations=10,\n",
    "    classes=classes,\n",
    "    log_basedir=f\"logs/fit_twostream_25_L10/{video_model.name}/\",\n",
    "    model_basedir=\"models/twostream_25_L10/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc5ad22",
   "metadata": {},
   "source": [
    "### Individual Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa39895",
   "metadata": {},
   "source": [
    "#### Video Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0109eba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=\"logs/fit_twostream_25_L10/video/\" + video_model.name + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"), \n",
    "    histogram_freq=1\n",
    ")\n",
    "\n",
    "video_model.fit(\n",
    "    video_train_gen, \n",
    "    epochs=1, \n",
    "    validation_data=video_test_gen,\n",
    "    callbacks=[vid_tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a10b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_model.save(\"models/twostream_25_L10/video/\" + video_model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fca552",
   "metadata": {},
   "source": [
    "#### Optflow Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf22b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optflow_tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=\"logs/fit_twostream_25_L10/optflow/\" + optflow_model.name + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"), \n",
    "    histogram_freq=1)\n",
    "\n",
    "optflow_model.fit(\n",
    "    optflow_train_gen, \n",
    "    epochs=1, \n",
    "    validation_data=optflow_test_gen,\n",
    "    callbacks=[optflow_tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e363c32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optflow_model.save(\"models/twostream_25_L10/optflow/\" + optflow_model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a981bc",
   "metadata": {},
   "source": [
    "#### TwoStream Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe446207",
   "metadata": {},
   "outputs": [],
   "source": [
    "twostream = Models.assemble_TwoStreamModel(video_model, optflow_model, 51, fusion=\"average\", recreate_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2389974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "twostream_tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=\"logs/fit_twostream_25_L10/twostream\" + twostream.name + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"), \n",
    "    histogram_freq=1)\n",
    "\n",
    "twostream.fit(\n",
    "    twostream_train_gen,\n",
    "    epochs=1,\n",
    "    validation_data=twostream_test_gen,\n",
    "    callbacks=[twostream_tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d97ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "twostream_resnet50v2.save(\"models/twostream_25_L1/ResNet50v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d57ad77",
   "metadata": {},
   "source": [
    "## Display attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101b563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = twostream_test_gen.__getitem__(4)\n",
    "x_video, x_optflow = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199507b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = Models.get_twostream_attention(x_video[0], twostream_resnet50v2)\n",
    "Models.video_to_gif(attention, \"./attention.gif\")\n",
    "\n",
    "Image(filename=\"./attention.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4183f18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradcam_attention = Models.get_twostream_gradcam(x_video[0], twostream_resnet50v2, \"conv5_block3_3_conv\")\n",
    "Models.video_to_gif(gradcam_attention, \"./gradcam_attention.gif\")\n",
    "\n",
    "Image(filename=\"./gradcam_attention.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567cfb27",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4473407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 1\n",
    "test_batch_size = 1\n",
    "\n",
    "lstm_train_gen = VideoDataGenerator.VideoDataGenerator(\n",
    "    dataset,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=train_batch_size,\n",
    "    preprocessing_function=tf.keras.applications.resnet_v2.preprocess_input,\n",
    "    rotation_range=20.0,\n",
    "    shear_range=20.0,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "lstm_test_gen = VideoDataGenerator.VideoDataGenerator(\n",
    "    dataset,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=test_batch_size,\n",
    "    preprocessing_function=tf.keras.applications.resnet_v2.preprocess_input\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b367a24",
   "metadata": {},
   "source": [
    "### Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befbc0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = tf.keras.applications.ResNet50V2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c1111",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172eadf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = Models.assemble_lstm(backbone, classes=51)#, recreate_top=True)\n",
    "\n",
    "lstm.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=25 * 10**-5, momentum=0.9, decay=0.0005),\n",
    "    metrics=[tf.keras.metrics.Accuracy(), tf.keras.metrics.TopKCategoricalAccuracy(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e2f13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=\"logs/lstm/\" + datasetname + \"/\" + lstm.name + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"), \n",
    "    histogram_freq=1)\n",
    "\n",
    "lstm.fit(\n",
    "    lstm_train_gen,\n",
    "    epochs=5,\n",
    "    #validation_data=lstm_test_gen,\n",
    "    callbacks=[lstm_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be8bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.evaluate(lstm_test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9de4cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.save(\"models/\" + datasetname + \"/\" + lstm.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4563c56d",
   "metadata": {},
   "source": [
    "## TODO: 3D-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f95a76",
   "metadata": {},
   "source": [
    "## TODO: (2+1)D-CNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "564.85px",
    "left": "1551px",
    "right": "20px",
    "top": "118px",
    "width": "352px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
